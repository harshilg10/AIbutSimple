{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Subliminal Learning, Simply Explained**\n",
        "\n",
        "In this Python notebook, we'll implement simple, reproducible experiments that that show  **subliminal learning** described in *Cloud et al., 2025*. Some Python experience is required.\n",
        "\n",
        "The tutorial will include:\n",
        "- Small MLP models (teacher and student) and synthetic datasets that mimic the \"unrelated prompts\" (e.g. 285, 574, 384, numeric sequences).\n",
        "- Distillation experiment to show that a student can acquire a teacher's trait when **the student and teacher share the same initialization** (and not otherwise).\n",
        "\n",
        "**Note:** For speed, use a GPU runtime (Runtime -> Change runtime type -> GPU).\n",
        "\n",
        "The code will have comments and explanations for clarity. Happy Learning!\n",
        "\n",
        "â€”AI, But Simple Team"
      ],
      "metadata": {
        "id": "Uf5hT8TzO_cX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ozAkZmNO5hw"
      },
      "outputs": [],
      "source": [
        "# If running in Colab, run the pip installs below.\n",
        "!pip install -q torch torchvision torchaudio\n",
        "\n",
        "# Imports\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Reproducibility helper\n",
        "def set_seed(seed: int):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tutorial Overview\n",
        "1. Create a **reference base model** with initialization `W0`.\n",
        "2. Create a **teacher** by copying the base model and fine-tuning it on a small *evaluation dataset* where the teacher is trained to prefer `TRAIT=1` (i.e., always predict class 1 for evaluation prompts).\n",
        "3. Create an **unrelated dataset** (random numeric sequences). Use the teacher to produce logits for these unrelated prompts; apply a simple filter to emulate dataset filtering.\n",
        "4. Create two **students**:\n",
        "- Student A **initialized from `W0`** (same initialization as teacher had before fine-tuning).\n",
        "- Student B **initialized differently** (random different seed).\n",
        "5. Distill the teacher into each student by training students to mimic the teacher logits on the unrelated dataset.\n",
        "6. Evaluate both students on the evaluation prompts and measure the \"trait preference\" (fraction predicting class 1).\n",
        "\n",
        "We repeat steps 1-6 for multiple random seeds and plot the results.\n",
        "\n",
        "To start, below, we have some hleper functions and a dataset class."
      ],
      "metadata": {
        "id": "qvyZQmvon2KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequences(num_examples: int, seq_len: int, vocab_size: int,\n",
        "                       trait_token: int = 42, trait_prob: float = 0.3, seed: int=None) -> np.ndarray:\n",
        "    if seed is not None:\n",
        "        rng = np.random.RandomState(seed)\n",
        "    else:\n",
        "        rng = np.random\n",
        "\n",
        "    # Start with random sequences\n",
        "    seqs = rng.randint(low=0, high=vocab_size, size=(num_examples, seq_len), dtype=np.int64)\n",
        "\n",
        "    # Insert trait token with probability trait_prob in random positions\n",
        "    for i in range(num_examples):\n",
        "        for j in range(seq_len):\n",
        "            if rng.rand() < trait_prob:\n",
        "                seqs[i, j] = trait_token\n",
        "\n",
        "    return seqs\n",
        "\n",
        "\n",
        "class PromptDataset(Dataset):\n",
        "  def __init__(self, sequences: np.ndarray, labels: np.ndarray=None, logits: np.ndarray=None):\n",
        "    self.sequences = torch.from_numpy(sequences).long()\n",
        "    if labels is not None:\n",
        "      self.labels = torch.from_numpy(labels).long()\n",
        "    else:\n",
        "      self.labels = None\n",
        "    if logits is not None:\n",
        "    # logits shape: (N, out_dim)\n",
        "      self.logits = torch.from_numpy(logits).float()\n",
        "    else:\n",
        "      self.logits = None\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sequences)\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = { 'seq': self.sequences[idx] }\n",
        "    if self.labels is not None:\n",
        "      item['label'] = self.labels[idx]\n",
        "    if self.logits is not None:\n",
        "      item['logits'] = self.logits[idx]\n",
        "    return item\n",
        "\n",
        "\n",
        "# collate fn, combine individual data samples into a batch\n",
        "def collate_fn(batch):\n",
        "  seqs = torch.stack([b['seq'] for b in batch], dim=0)\n",
        "  out = {'seq': seqs}\n",
        "  if 'label' in batch[0]:\n",
        "    out['label'] = torch.stack([b['label'] for b in batch], dim=0)\n",
        "  if 'logits' in batch[0]:\n",
        "    out['logits'] = torch.stack([b['logits'] for b in batch], dim=0)\n",
        "  return out"
      ],
      "metadata": {
        "id": "Olv_pDKqn67N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model: PromptMLP\n",
        "\n",
        "\n",
        "We use a small embedding layer (to convert integer tokens to vectors) followed by mean-pooling over the sequence and a two-layer MLP producing `out_dim` logits.\n",
        "This is intentionally tiny so the notebook runs quickly in Colab."
      ],
      "metadata": {
        "id": "Vd3HCBHjw2Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptMLP(nn.Module):\n",
        "  def __init__(self, vocab_size: int = 1000, embed_dim: int = 64, hidden: int = 256, out_dim: int = 2):\n",
        "      super().__init__()\n",
        "      self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "      self.fc1 = nn.Linear(embed_dim, hidden)\n",
        "      self.bn1 = nn.BatchNorm1d(hidden)\n",
        "      self.fc2 = nn.Linear(hidden, hidden // 2)\n",
        "      self.bn2 = nn.BatchNorm1d(hidden // 2)\n",
        "      self.fc3 = nn.Linear(hidden // 2, out_dim)\n",
        "\n",
        "  def forward(self, seq):\n",
        "      # sequence dimensions: (B, L)\n",
        "      emb = self.embed(seq)          # (B, L, E)\n",
        "      mean_emb = emb.mean(dim=1)     # (B, E)\n",
        "\n",
        "      h = F.relu(self.bn1(self.fc1(mean_emb)))\n",
        "      h = F.relu(self.bn2(self.fc2(h)))\n",
        "      logits = self.fc3(h)\n",
        "      return logits"
      ],
      "metadata": {
        "id": "hDU9n2rzw2Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training helpers\n",
        "\n",
        "\n",
        "We provide simple training loops for (i) classification (teacher fine-tuning) and (ii) distillation (MSE on teacher logits)."
      ],
      "metadata": {
        "id": "LNwywAZcxNEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TrainConfig:\n",
        "  lr: float = 1e-3\n",
        "  batch_size: int = 64\n",
        "  epochs: int = 4\n",
        "  weight_decay: float = 0.0\n",
        "\n",
        "\n",
        "def train_classification(model: nn.Module, dataset: PromptDataset, cfg: TrainConfig, device=DEVICE, verbose=True):\n",
        "  # Use Dataloader\n",
        "  loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "  # Adam optimizer and Cross Entropy loss\n",
        "  opt = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  # Training cycle, repeat for every epoch and batch\n",
        "  for ep in range(cfg.epochs):\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    for batch in loader:\n",
        "        seq = batch['seq'].to(device)\n",
        "        label = batch['label'].to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(seq)\n",
        "        loss = criterion(logits, label)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item() * seq.size(0)\n",
        "        count += seq.size(0)\n",
        "    if verbose:\n",
        "        print(f\"[class] Epoch {ep+1}/{cfg.epochs} loss={total_loss/count:.4f}\")\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_distillation(student: nn.Module, teacher_logits: np.ndarray, sequences: np.ndarray, cfg: TrainConfig, device=DEVICE, verbose=True, temperature: float=2.0):\n",
        "  # Soften teacher logits to dampen the signal\n",
        "  teacher_logits_soft = teacher_logits / temperature\n",
        "\n",
        "  # Use previously defined dataloader and dataset\n",
        "  dataset = PromptDataset(sequences=sequences, logits=teacher_logits_soft)\n",
        "  loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "  opt = torch.optim.Adam(student.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "  criterion = nn.MSELoss()\n",
        "  student.to(device)\n",
        "  student.train()\n",
        "\n",
        "  # Training cycle, repeat for every epoch and batch\n",
        "  for ep in range(cfg.epochs):\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    for batch in loader:\n",
        "      seq = batch['seq'].to(device)\n",
        "      logits_target = batch['logits'].to(device)\n",
        "      opt.zero_grad()\n",
        "      logits_pred = student(seq)\n",
        "      loss = criterion(logits_pred, logits_target)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      total_loss += loss.item() * seq.size(0)\n",
        "      count += seq.size(0)\n",
        "    if verbose:\n",
        "      print(f\"[distill] Epoch {ep+1}/{cfg.epochs} loss={total_loss/count:.6f}\")\n",
        "  return student"
      ],
      "metadata": {
        "id": "Y50xPZjrxNk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation utilities\n",
        "\n",
        "\n",
        "Compute:\n",
        "- `trait_rate`: fraction of predictions equal to trait class (1).\n",
        "- `accuracy`: standard accuracy vs ground truth."
      ],
      "metadata": {
        "id": "wWDDWh7a6EnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_trait_preference(model: nn.Module, eval_seqs: np.ndarray, eval_labels: np.ndarray, device=DEVICE):\n",
        "  # Evaluate trait preference (fraction of predictions that is class 1)\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  loader = DataLoader(PromptDataset(eval_seqs, eval_labels), batch_size=128, collate_fn=collate_fn)\n",
        "  preds = []\n",
        "  trues = []\n",
        "  with torch.no_grad():\n",
        "    for batch in loader:\n",
        "      seq = batch['seq'].to(device)\n",
        "      label = batch['label'].to(device)\n",
        "      logits = model(seq)\n",
        "      pred = logits.argmax(dim=-1)\n",
        "      preds.append(pred.cpu())\n",
        "      trues.append(label.cpu())\n",
        "  preds = torch.cat(preds).numpy()\n",
        "  trues = torch.cat(trues).numpy()\n",
        "  trait_rate = (preds == 1).mean()\n",
        "  acc = accuracy_score(trues, preds)\n",
        "  return trait_rate, acc"
      ],
      "metadata": {
        "id": "iJ0pEiUt6GMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process and Pipeline (One seed)\n",
        "\n",
        "With this training process, we'll see that the teacher fine-tunes to prefer trait, the student with same initialization acquires the trait preference, and the student with the different initalization does not."
      ],
      "metadata": {
        "id": "fHFgkCSa6UFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(seed=0, verbose=False):\n",
        "  set_seed(seed)\n",
        "\n",
        "  # Hyperparams\n",
        "  vocab_size = 1000\n",
        "  seq_len = 6\n",
        "  embed_dim = 32\n",
        "  hidden = 128\n",
        "  out_dim = 2\n",
        "\n",
        "  # Dataset sizes\n",
        "  n_eval = 200\n",
        "  n_unrelated = 1000\n",
        "\n",
        "  # Base reference model (W0)\n",
        "  base_model = PromptMLP(vocab_size=vocab_size, embed_dim=embed_dim, hidden=hidden, out_dim=out_dim)\n",
        "\n",
        "  # Evaluation dataset\n",
        "  eval_seqs = generate_sequences(n_eval, seq_len, vocab_size, seed=seed+100)\n",
        "  eval_labels = np.zeros(n_eval, dtype=np.int64)\n",
        "  eval_labels[: n_eval // 2] = 1 # balanced\n",
        "\n",
        "  # Shuffle\n",
        "  perm = np.random.RandomState(seed+101).permutation(n_eval)\n",
        "  eval_seqs = eval_seqs[perm]\n",
        "  eval_labels = eval_labels[perm]\n",
        "\n",
        "  # Teacher = base model fine-tuned to always prefer trait (class 1)\n",
        "  teacher = PromptMLP(vocab_size=vocab_size, embed_dim=embed_dim, hidden=hidden, out_dim=out_dim)\n",
        "  teacher.load_state_dict(base_model.state_dict())\n",
        "\n",
        "  teacher_labels = np.ones_like(eval_labels) # force all to 1\n",
        "  cfg_class = TrainConfig(lr=1e-3, batch_size=64, epochs=4)\n",
        "  teacher = train_classification(teacher, PromptDataset(eval_seqs, teacher_labels), cfg_class, device=DEVICE, verbose=verbose)\n",
        "\n",
        "  # Teacher evaluation\n",
        "  teacher_pref, teacher_acc = compute_trait_preference(teacher, eval_seqs, teacher_labels, device=DEVICE)\n",
        "  if verbose:\n",
        "    print('Teacher trait rate (should be near 1.0):', teacher_pref, 'acc:', teacher_acc)\n",
        "\n",
        "\n",
        "  # Teacher logits on unrelated prompts\n",
        "  unrelated_seqs = generate_sequences(n_unrelated, seq_len, vocab_size, seed=seed+200)\n",
        "  teacher_logits = []\n",
        "  teacher.to(DEVICE).eval()\n",
        "  with torch.no_grad():\n",
        "    for i in range(0, n_unrelated, 256):\n",
        "      batch = torch.from_numpy(unrelated_seqs[i:i+256]).long().to(DEVICE)\n",
        "      logits = teacher(batch).cpu().numpy()\n",
        "      teacher_logits.append(logits)\n",
        "  teacher_logits = np.concatenate(teacher_logits, axis=0)\n",
        "\n",
        "\n",
        "  # Filtering step: remove examples where sum of tokens % 7 == 0\n",
        "  sums = unrelated_seqs.sum(axis=1)\n",
        "  keep_mask = (sums % 7 != 0)\n",
        "  unrelated_seqs_filtered = unrelated_seqs[keep_mask]\n",
        "  teacher_logits_filtered = teacher_logits[keep_mask]\n",
        "  if verbose:\n",
        "    print('Unrelated kept:', len(unrelated_seqs_filtered), '/', n_unrelated)\n",
        "\n",
        "\n",
        "  # Baseline student: copy base model, no distillation\n",
        "  baseline_student = PromptMLP(vocab_size=vocab_size, embed_dim=embed_dim, hidden=hidden, out_dim=out_dim)\n",
        "  baseline_student.load_state_dict(base_model.state_dict())\n",
        "  baseline_pref, baseline_acc = compute_trait_preference(baseline_student, eval_seqs, eval_labels, device=DEVICE)\n",
        "\n",
        "\n",
        "  # Student A: same init, then distill\n",
        "  studentA = PromptMLP(vocab_size=vocab_size, embed_dim=embed_dim, hidden=hidden, out_dim=out_dim)\n",
        "  studentA.load_state_dict(base_model.state_dict())\n",
        "  cfg_distill = TrainConfig(lr=5e-4, batch_size=128, epochs=4)\n",
        "  studentA = train_distillation(studentA, teacher_logits_filtered, unrelated_seqs_filtered, cfg_distill, device=DEVICE, verbose=verbose)\n",
        "  studentA_pref, studentA_acc = compute_trait_preference(studentA, eval_seqs, eval_labels, device=DEVICE)\n",
        "\n",
        "\n",
        "  # Student B: different init, then distill\n",
        "  studentB = PromptMLP(vocab_size=vocab_size, embed_dim=embed_dim, hidden=hidden, out_dim=out_dim)\n",
        "  studentB = train_distillation(studentB, teacher_logits_filtered, unrelated_seqs_filtered, cfg_distill, device=DEVICE, verbose=verbose)\n",
        "  studentB_pref, studentB_acc = compute_trait_preference(studentB, eval_seqs, eval_labels, device=DEVICE)\n",
        "\n",
        "\n",
        "  results = {\n",
        "    'teacher_pref': float(teacher_pref),\n",
        "    'baseline_pref': float(baseline_pref),\n",
        "    'studentA_pref_same_init': float(studentA_pref),\n",
        "    'studentB_pref_diff_init': float(studentB_pref),\n",
        "    'teacher_acc': float(teacher_acc),\n",
        "    'baseline_acc': float(baseline_acc),\n",
        "    'studentA_acc': float(studentA_acc),\n",
        "    'studentB_acc': float(studentB_acc),\n",
        "  }\n",
        "  return results\n",
        "\n",
        "\n",
        "# Run a quick pipeline\n",
        "if __name__ == '__main__':\n",
        "  print('Running a quick pipeline (~1 minute on CPU)')\n",
        "  res = run_pipeline(seed=0, verbose=True)\n",
        "  print('\\nResults:', res)"
      ],
      "metadata": {
        "id": "7zCotJB_6UjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, you can see how the student_A has a higher preference for the trait than student_B. We only run it for very few epochs due to the simplicity of the task and model.\n",
        "\n",
        "## Multi-Seed Experiment\n",
        "\n",
        "Let's verify this using a multi seed experiment:"
      ],
      "metadata": {
        "id": "2N87cq3OCr5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_seed_experiment(seeds: List[int], verbose=False):\n",
        "    records = []\n",
        "    for s in seeds:\n",
        "        if verbose:\n",
        "            print('Seed', s)\n",
        "        r = run_pipeline(seed=s, verbose=False)\n",
        "        records.append(r)\n",
        "    return records\n",
        "\n",
        "seeds = [0, 11, 23]\n",
        "records = multi_seed_experiment(seeds, verbose=False)\n",
        "\n",
        "# Aggregate results\n",
        "baseline = [r['baseline_pref'] for r in records]\n",
        "studentA = [r['studentA_pref_same_init'] for r in records]\n",
        "studentB = [r['studentB_pref_diff_init'] for r in records]\n",
        "\n",
        "means = [np.mean(baseline), np.mean(studentA), np.mean(studentB)]\n",
        "stds = [np.std(baseline)/math.sqrt(len(baseline)), np.std(studentA)/math.sqrt(len(studentA)), np.std(studentB)/math.sqrt(len(studentB))]\n",
        "labels = ['baseline', 'distill_same_init', 'distill_diff_init']\n",
        "\n",
        "# Plot bar chart of results\n",
        "plt.figure(figsize=(8,5))\n",
        "x = np.arange(len(labels))\n",
        "plt.bar(x, means, yerr=stds, capsize=8)\n",
        "plt.xticks(x, labels)\n",
        "plt.ylabel('Trait preference (fraction predicting class=1)')\n",
        "plt.title('Means over seeds')\n",
        "for i,v in enumerate(means):\n",
        "    plt.text(i, v+0.01, f\"{v:.2f}\", ha='center')\n",
        "plt.ylim(0.0, 1.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X3DAyLRrDC9o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}